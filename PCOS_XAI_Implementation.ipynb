{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgXb1pE1RuBE",
        "outputId": "43f110e3-60d5-43ef-bccc-a3470c703d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torchvision torch pytorch-grad-cam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G3ALUFFs6nK",
        "outputId": "593bbec4-6db5-4712-8198-51959236cd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.5.3.tar.gz (7.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/7.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m5.5/7.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (10.4.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.19.1+cu121)\n",
            "Collecting ttach (from grad-cam)\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.66.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.3-py3-none-any.whl size=38657 sha256=b4a8a102eb79371f7548fc29b5ffc090a33fdc4da43fd228a3bd07aeacc354f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/ce/70/fe64f851895eae830b3c63ec7fc464cfa7c81aeb7ad4f68063\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.5.3 ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
      ],
      "metadata": {
        "id": "hmK4G1vPR1pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZgLDm8tr2IX",
        "outputId": "0c77af19-a4c8-4e0e-af73-8a5e1b058c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_folder = '/content/drive/MyDrive/pocs/'\n",
        "infected_folder = img_folder + 'infected/'\n",
        "non_infected_folder = img_folder + 'notinfected/'"
      ],
      "metadata": {
        "id": "tSjRvKver28o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_paths = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "gVrGYbLvr6G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 folders 'infected' and 'non_infected'\n",
        "for img_name in os.listdir(infected_folder):\n",
        "    img_paths.append(os.path.join(infected_folder, img_name))\n",
        "    labels.append(1)  # Label 1 for infected"
      ],
      "metadata": {
        "id": "KDA_CH3or6sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# non-infected images\n",
        "for img_name in os.listdir(non_infected_folder):\n",
        "    img_paths.append(os.path.join(non_infected_folder, img_name))\n",
        "    labels.append(0)  # Label 0 for non-infected"
      ],
      "metadata": {
        "id": "wtR9w-k1sJQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PCOS class\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert('RGB') #image is loaded as RGB\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "EAZRVUjbR5OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "Pe6jDs1esVgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = MedicalDataset(img_paths, labels, transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "8jo_tTXbsbI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# DenseNet201 model\n",
        "model = models.densenet201(pretrained=True)\n",
        "model.classifier = nn.Linear(1920, 2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "yvwTV9DuR79i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed8cecc-3216-464e-e1ca-9d4eafdd829d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
            "100%|██████████| 77.4M/77.4M [00:00<00:00, 92.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader)}')\n"
      ],
      "metadata": {
        "id": "TmTlRVvNSCzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a500492c-e78a-4dae-ea05-0f057d759099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Loss: 0.008696798825425022\n",
            "Epoch [2/25], Loss: 3.6285243802260744e-05\n",
            "Epoch [3/25], Loss: 1.8857313245296776e-05\n",
            "Epoch [4/25], Loss: 9.7402877269236e-06\n",
            "Epoch [5/25], Loss: 1.2410838213921027e-05\n",
            "Epoch [6/25], Loss: 3.1317088961026275e-06\n",
            "Epoch [7/25], Loss: 3.552772355302175e-06\n",
            "Epoch [8/25], Loss: 5.892983187958041e-06\n",
            "Epoch [9/25], Loss: 2.7944125675234814e-06\n",
            "Epoch [10/25], Loss: 3.995269817650755e-06\n",
            "Epoch [11/25], Loss: 0.048552717904629464\n",
            "Epoch [12/25], Loss: 0.07232444973024106\n",
            "Epoch [13/25], Loss: 0.004662744542528802\n",
            "Epoch [14/25], Loss: 0.0004594754402307993\n",
            "Epoch [15/25], Loss: 0.00011014844658897654\n",
            "Epoch [16/25], Loss: 0.0028346094190767925\n",
            "Epoch [17/25], Loss: 0.006244362644497508\n",
            "Epoch [18/25], Loss: 0.0010247693672824447\n",
            "Epoch [19/25], Loss: 6.207527922553186e-05\n",
            "Epoch [20/25], Loss: 1.8780496658486296e-05\n",
            "Epoch [21/25], Loss: 0.0034570761333011907\n",
            "Epoch [22/25], Loss: 0.021756884249404442\n",
            "Epoch [23/25], Loss: 0.0005818093151680112\n",
            "Epoch [24/25], Loss: 0.002368226640164947\n",
            "Epoch [25/25], Loss: 0.0003009674460579183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_layers = [model.features.denseblock4]\n",
        "cam = GradCAM(model=model, target_layers=target_layers)"
      ],
      "metadata": {
        "id": "gpXOl4h7nUP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (8).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "pUrQMQvnuPw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = [ClassifierOutputTarget(1)]\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n"
      ],
      "metadata": {
        "id": "6pYbSQVJuRV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img)\n",
        "if len(img_np.shape) == 2:\n",
        "    img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "img_np = np.uint8(255 * img_np / np.max(img_np))\n",
        "\n",
        "if len(grayscale_cam.shape) == 3:\n",
        "    grayscale_cam = grayscale_cam[:, :, 0]\n",
        "grayscale_cam = np.uint8(255 * grayscale_cam)\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)"
      ],
      "metadata": {
        "id": "1AQk2tYzngsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2A7PZlIluT8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Grad-CAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DZrPH5zkbAhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying on diffrent data sample _ Infected"
      ],
      "metadata": {
        "id": "gsZzkymbzmwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "w4sqA4J6y9Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = [ClassifierOutputTarget(1)]\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "if len(grayscale_cam.shape) == 2:\n",
        "    grayscale_cam = grayscale_cam[..., np.newaxis]"
      ],
      "metadata": {
        "id": "TyER5tl9y9Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0, :], use_rgb=True)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Grad-CAM')\n",
        "plt.imshow(cam_image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HJ7Bki6Ly9Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(28).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "targets = [ClassifierOutputTarget(1)]\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "\n",
        "if len(grayscale_cam.shape) == 2:\n",
        "    grayscale_cam = grayscale_cam[..., np.newaxis]\n",
        "\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0, :], use_rgb=True)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Grad-CAM')\n",
        "plt.imshow(cam_image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1m4QSCH-zVBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ScoreCAM\n"
      ],
      "metadata": {
        "id": "3GKgOYhP_fHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam\n",
        "\n",
        "from pytorch_grad_cam import ScoreCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(28).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "cam = ScoreCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "targets = [ClassifierOutputTarget(1)]\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "if len(grayscale_cam.shape) == 2:\n",
        "    grayscale_cam = grayscale_cam[..., np.newaxis]\n",
        "\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0, :], use_rgb=True)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Score-CAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o9yPKKVp0mC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GradCAMPlusPlus"
      ],
      "metadata": {
        "id": "QENoxzuM_q_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam\n",
        "\n",
        "from pytorch_grad_cam import GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(28).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "cam = GradCAMPlusPlus(model=model, target_layers=target_layers)\n",
        "\n",
        "targets = [ClassifierOutputTarget(1)]\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Gradcam++')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rvbKjaA41GUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGradCAM"
      ],
      "metadata": {
        "id": "idwVt4sz_vDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import XGradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "cam = XGradCAM(model=model, target_layers=target_layers)\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('XGradCAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i3KFZLHM1o_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EigenCAM"
      ],
      "metadata": {
        "id": "1kApDb4Q_yaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import EigenCAM\n",
        "\n",
        "cam = EigenCAM(model=model, target_layers=target_layers)\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('EigenCAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zNQB8SSD18pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LayerCAM"
      ],
      "metadata": {
        "id": "b1RBf6lS_3jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import LayerCAM\n",
        "\n",
        "cam = LayerCAM(model=model, target_layers=target_layers)\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('LayerCAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pfgvzMBT29c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full-Gradient /  FullGrad"
      ],
      "metadata": {
        "id": "nNYUhqSx5zLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import FullGrad\n",
        "\n",
        "cam = FullGrad(model=model, target_layers=target_layers)\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Full-Gradient')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uST3xDL75Xda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Thresholded CAM (Binary Mask)\n"
      ],
      "metadata": {
        "id": "pSRhoyd36vFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "threshold = 0.5\n",
        "binary_cam = (grayscale_cam[0] > threshold).astype(float)\n",
        "cam_image = show_cam_on_image(img_np, binary_cam, use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Thresholded CAM ')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aHPdEm8r6Nbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img)\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "def predict_fn(images):\n",
        "    inputs = [transform(Image.fromarray(image)).unsqueeze(0).to(device) for image in images]\n",
        "    inputs = torch.cat(inputs, dim=0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "    return outputs.cpu().numpy()\n",
        "\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "explanation = explainer.explain_instance(img_np,\n",
        "                                         classifier_fn=predict_fn,\n",
        "                                         top_labels=2,\n",
        "                                         hide_color=0,\n",
        "                                         num_samples=1000)\n",
        "\n",
        "temp, mask = explanation.get_image_and_mask(label=1, positive_only=False, num_features=10, hide_rest=False)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('LIME Explanation (Infected)')\n",
        "plt.imshow(mark_boundaries(temp, mask))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oeV0qhqwJYHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OcclusionSensitivity"
      ],
      "metadata": {
        "id": "Jcd7LqaeACkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. SHAP (Shapley Additive exPlanations)"
      ],
      "metadata": {
        "id": "Jbvyb_VzW6--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "explainer = shap.DeepExplainer(model, input_tensor)\n",
        "shap_values = explainer.shap_values(input_tensor)\n",
        "shap.image_plot(shap_values, np.array([img_np]))\n"
      ],
      "metadata": {
        "id": "mgXuedIR-bBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "def apply_occlusion(image, size=32, stride=8):\n",
        "    heatmap = np.zeros((image.shape[1], image.shape[2]))\n",
        "    for y in range(0, image.shape[1] - size + 1, stride):\n",
        "        for x in range(0, image.shape[2] - size + 1, stride):\n",
        "            occluded_image = image.clone()\n",
        "            occluded_image[:, :, y:y+size, x:x+size] = 0\n",
        "            output = model(occluded_image).squeeze()\n",
        "            heatmap[y:y+size, x:x+size] = output[1].item()\n",
        "    return heatmap\n",
        "\n",
        "occlusion_heatmap = apply_occlusion(input_tensor)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Occlusion Sensitivity')\n",
        "plt.imshow(occlusion_heatmap, cmap='jet')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SML7Hs9WXA0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from captum.attr import IntegratedGradients\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "ig = IntegratedGradients(model)\n",
        "attributions = ig.attribute(input_tensor, target=1)\n",
        "attributions = attributions.squeeze().cpu().detach().numpy()\n",
        "\n",
        "attributions = np.sum(np.abs(attributions), axis=0)\n",
        "attributions = (attributions - np.min(attributions)) / (np.max(attributions) - np.min(attributions))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Integrated Gradients')\n",
        "plt.imshow(attributions, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "waQloE2NXAwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def add_noise(image, noise_level=0.1):\n",
        "    noise = noise_level * torch.randn_like(image).to(device)\n",
        "    return image + noise\n",
        "\n",
        "def smoothgrad(input_tensor, n_samples=50, noise_level=0.1):\n",
        "    smoothed_grad = torch.zeros_like(input_tensor)\n",
        "    for i in range(n_samples):\n",
        "        noisy_input = add_noise(input_tensor, noise_level)\n",
        "        noisy_input.requires_grad = True\n",
        "        output = model(noisy_input).squeeze()[1]\n",
        "        output.backward()\n",
        "        smoothed_grad += noisy_input.grad.abs()\n",
        "    return smoothed_grad / n_samples\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "smoothed_grad = smoothgrad(input_tensor).squeeze().cpu().detach().numpy()\n",
        "smoothed_grad = np.sum(np.abs(smoothed_grad), axis=0)\n",
        "smoothed_grad = (smoothed_grad - np.min(smoothed_grad)) / (np.max(smoothed_grad) - np.min(smoothed_grad))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('SmoothGrad')\n",
        "plt.imshow(smoothed_grad, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YZqO5gSJXHJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import GuidedBackprop\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "guided_bp = GuidedBackprop(model)\n",
        "attributions = guided_bp.attribute(input_tensor, target=1)\n",
        "attributions = attributions.squeeze().cpu().detach().numpy()\n",
        "attributions = np.sum(attributions, axis=0)\n",
        "attributions = np.maximum(attributions, 0)\n",
        "attributions = (attributions - attributions.min()) / (attributions.max() - attributions.min())\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Guided Backpropagation')\n",
        "plt.imshow(attributions, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t43xgzRvXJl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output = model(input_tensor)\n",
        "    output_score = output[0, 1].item()\n",
        "\n",
        "def score_features(input_tensor, score_function):\n",
        "    scores = np.zeros(input_tensor.shape[-2:])\n",
        "    for y in range(input_tensor.shape[-2]):\n",
        "        for x in range(input_tensor.shape[-1]):\n",
        "            perturbed_input = input_tensor.clone()\n",
        "            perturbed_input[:, :, y, x] = 0\n",
        "            perturbed_output = model(perturbed_input)[0, 1].item()\n",
        "            scores[y, x] = output_score - perturbed_output\n",
        "    return scores\n",
        "\n",
        "scores = score_features(input_tensor, output_score)\n",
        "scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('SBE Heatmap')\n",
        "plt.imshow(scores, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dnhcvY-RXZL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1_(13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "def apply_perturbation(img_np, patch_size=32, stride=8):\n",
        "    perturbed_img = img.copy()\n",
        "    for y in range(0, img_np.shape[0] - patch_size + 1, stride):\n",
        "        for x in range(0, img_np.shape[1] - patch_size + 1, stride):\n",
        "            patch = img.crop((x, y, x + patch_size, y + patch_size))\n",
        "            blurred_patch = patch.filter(ImageFilter.GaussianBlur(radius=5))\n",
        "            perturbed_img.paste(blurred_patch, (x, y))\n",
        "    return np.array(perturbed_img) / 255.0\n",
        "\n",
        "perturbed_img_np = apply_perturbation(img_np)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Perturbation-Based Saliency Map')\n",
        "plt.imshow(perturbed_img_np)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GbtAaU0IXx-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}